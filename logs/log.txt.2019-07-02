2019-07-02 21:45:29,096 [main] INFO [org.apache.spark.SparkContext] - Running Spark version 2.2.0
2019-07-02 21:45:29,834 [main] WARN [org.apache.hadoop.util.NativeCodeLoader] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-07-02 21:45:30,732 [main] INFO [org.apache.spark.SparkContext] - Submitted application: UDF
2019-07-02 21:45:30,775 [main] INFO [org.apache.spark.SecurityManager] - Changing view acls to: FrankCooper
2019-07-02 21:45:30,777 [main] INFO [org.apache.spark.SecurityManager] - Changing modify acls to: FrankCooper
2019-07-02 21:45:30,778 [main] INFO [org.apache.spark.SecurityManager] - Changing view acls groups to: 
2019-07-02 21:45:30,778 [main] INFO [org.apache.spark.SecurityManager] - Changing modify acls groups to: 
2019-07-02 21:45:30,780 [main] INFO [org.apache.spark.SecurityManager] - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(FrankCooper); groups with view permissions: Set(); users  with modify permissions: Set(FrankCooper); groups with modify permissions: Set()
2019-07-02 21:45:33,757 [main] INFO [org.apache.spark.util.Utils] - Successfully started service 'sparkDriver' on port 60431.
2019-07-02 21:45:33,779 [main] INFO [org.apache.spark.SparkEnv] - Registering MapOutputTracker
2019-07-02 21:45:33,799 [main] INFO [org.apache.spark.SparkEnv] - Registering BlockManagerMaster
2019-07-02 21:45:33,821 [main] INFO [org.apache.spark.storage.BlockManagerMasterEndpoint] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-07-02 21:45:33,822 [main] INFO [org.apache.spark.storage.BlockManagerMasterEndpoint] - BlockManagerMasterEndpoint up
2019-07-02 21:45:33,844 [main] INFO [org.apache.spark.storage.DiskBlockManager] - Created local directory at C:\Users\FrankCooper\AppData\Local\Temp\blockmgr-a9720f4c-9497-46c7-b710-69ef7fedf280
2019-07-02 21:45:33,916 [main] INFO [org.apache.spark.storage.memory.MemoryStore] - MemoryStore started with capacity 1989.6 MB
2019-07-02 21:45:34,041 [main] INFO [org.apache.spark.SparkEnv] - Registering OutputCommitCoordinator
2019-07-02 21:45:34,161 [main] INFO [org.spark_project.jetty.util.log] - Logging initialized @6541ms
2019-07-02 21:45:34,280 [main] INFO [org.spark_project.jetty.server.Server] - jetty-9.3.z-SNAPSHOT
2019-07-02 21:45:34,294 [main] INFO [org.spark_project.jetty.server.Server] - Started @6673ms
2019-07-02 21:45:34,338 [main] INFO [org.spark_project.jetty.server.AbstractConnector] - Started ServerConnector@f1a45f8{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-02 21:45:34,338 [main] INFO [org.apache.spark.util.Utils] - Successfully started service 'SparkUI' on port 4040.
2019-07-02 21:45:34,365 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1f193686{/jobs,null,AVAILABLE,@Spark}
2019-07-02 21:45:34,367 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@67e28be3{/jobs/json,null,AVAILABLE,@Spark}
2019-07-02 21:45:34,368 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@4d48bd85{/jobs/job,null,AVAILABLE,@Spark}
2019-07-02 21:45:34,369 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@2ff95fc6{/jobs/job/json,null,AVAILABLE,@Spark}
2019-07-02 21:45:34,370 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@31c2affc{/stages,null,AVAILABLE,@Spark}
2019-07-02 21:45:34,371 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@7c4fc2bf{/stages/json,null,AVAILABLE,@Spark}
2019-07-02 21:45:34,373 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@236134a1{/stages/stage,null,AVAILABLE,@Spark}
2019-07-02 21:45:34,374 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@68dcfd52{/stages/stage/json,null,AVAILABLE,@Spark}
2019-07-02 21:45:34,376 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@7d37f1c{/stages/pool,null,AVAILABLE,@Spark}
2019-07-02 21:45:34,377 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@3047254d{/stages/pool/json,null,AVAILABLE,@Spark}
2019-07-02 21:45:34,378 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@c6e0f32{/storage,null,AVAILABLE,@Spark}
2019-07-02 21:45:34,380 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@63fdffcd{/storage/json,null,AVAILABLE,@Spark}
2019-07-02 21:45:34,382 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@748e9b20{/storage/rdd,null,AVAILABLE,@Spark}
2019-07-02 21:45:34,384 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@50a3d0f6{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-07-02 21:45:34,385 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@3e4f80cb{/environment,null,AVAILABLE,@Spark}
2019-07-02 21:45:34,386 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@c6a6c1d{/environment/json,null,AVAILABLE,@Spark}
2019-07-02 21:45:34,388 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@77c7ed8e{/executors,null,AVAILABLE,@Spark}
2019-07-02 21:45:34,389 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@640dc4c6{/executors/json,null,AVAILABLE,@Spark}
2019-07-02 21:45:34,390 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@44de94c3{/executors/threadDump,null,AVAILABLE,@Spark}
2019-07-02 21:45:34,391 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@11de56e6{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-07-02 21:45:34,401 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@4441d567{/static,null,AVAILABLE,@Spark}
2019-07-02 21:45:34,403 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@26a4551a{/,null,AVAILABLE,@Spark}
2019-07-02 21:45:34,404 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@73ad4ecc{/api,null,AVAILABLE,@Spark}
2019-07-02 21:45:34,405 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@7ce4de34{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-07-02 21:45:34,407 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@acf859d{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-07-02 21:45:34,409 [main] INFO [org.apache.spark.ui.SparkUI] - Bound SparkUI to 0.0.0.0, and started at http://192.168.200.1:4040
2019-07-02 21:45:34,502 [main] INFO [org.apache.spark.executor.Executor] - Starting executor ID driver on host localhost
2019-07-02 21:45:34,545 [main] INFO [org.apache.spark.util.Utils] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 60440.
2019-07-02 21:45:34,545 [main] INFO [org.apache.spark.network.netty.NettyBlockTransferService] - Server created on 192.168.200.1:60440
2019-07-02 21:45:34,547 [main] INFO [org.apache.spark.storage.BlockManager] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-07-02 21:45:34,550 [main] INFO [org.apache.spark.storage.BlockManagerMaster] - Registering BlockManager BlockManagerId(driver, 192.168.200.1, 60440, None)
2019-07-02 21:45:34,553 [dispatcher-event-loop-2] INFO [org.apache.spark.storage.BlockManagerMasterEndpoint] - Registering block manager 192.168.200.1:60440 with 1989.6 MB RAM, BlockManagerId(driver, 192.168.200.1, 60440, None)
2019-07-02 21:45:34,556 [main] INFO [org.apache.spark.storage.BlockManagerMaster] - Registered BlockManager BlockManagerId(driver, 192.168.200.1, 60440, None)
2019-07-02 21:45:34,556 [main] INFO [org.apache.spark.storage.BlockManager] - Initialized BlockManager: BlockManagerId(driver, 192.168.200.1, 60440, None)
2019-07-02 21:45:34,750 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@6b1dc20f{/metrics/json,null,AVAILABLE,@Spark}
2019-07-02 21:46:41,178 [main] INFO [org.apache.spark.sql.internal.SharedState] - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/E:/StudyingCourse/IntellijModule/tech-climbing-common/spark-warehouse').
2019-07-02 21:46:41,186 [main] INFO [org.apache.spark.sql.internal.SharedState] - Warehouse path is 'file:/E:/StudyingCourse/IntellijModule/tech-climbing-common/spark-warehouse'.
2019-07-02 21:46:41,227 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@148ab138{/SQL,null,AVAILABLE,@Spark}
2019-07-02 21:46:41,241 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@131ba005{/SQL/json,null,AVAILABLE,@Spark}
2019-07-02 21:46:41,258 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@5e158f33{/SQL/execution,null,AVAILABLE,@Spark}
2019-07-02 21:46:41,273 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@3ae2702a{/SQL/execution/json,null,AVAILABLE,@Spark}
2019-07-02 21:46:41,296 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@6a4a2090{/static/sql,null,AVAILABLE,@Spark}
2019-07-02 21:46:44,536 [main] INFO [org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef] - Registered StateStoreCoordinator endpoint
2019-07-02 21:46:46,913 [main] INFO [org.apache.spark.sql.execution.SparkSqlParser] - Parsing command: names
2019-07-02 21:47:01,238 [main] INFO [org.apache.spark.sql.execution.SparkSqlParser] - Parsing command: select name,strLen(name) from names
2019-07-02 21:47:09,234 [main] INFO [org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator] - Code generated in 5140.8044 ms
2019-07-02 21:47:09,754 [main] INFO [org.apache.spark.SparkContext] - Starting job: collect at UDF.scala:39
2019-07-02 21:47:09,802 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Got job 0 (collect at UDF.scala:39) with 5 output partitions
2019-07-02 21:47:09,803 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 0 (collect at UDF.scala:39)
2019-07-02 21:47:09,803 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2019-07-02 21:47:09,805 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2019-07-02 21:47:09,836 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 0 (MapPartitionsRDD[5] at collect at UDF.scala:39), which has no missing parents
2019-07-02 21:47:10,092 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_0 stored as values in memory (estimated size 12.7 KB, free 1989.6 MB)
2019-07-02 21:47:10,306 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 6.3 KB, free 1989.6 MB)
2019-07-02 21:47:10,322 [dispatcher-event-loop-0] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_0_piece0 in memory on 192.168.200.1:60440 (size: 6.3 KB, free: 1989.6 MB)
2019-07-02 21:47:10,355 [dag-scheduler-event-loop] INFO [org.apache.spark.SparkContext] - Created broadcast 0 from broadcast at DAGScheduler.scala:1006
2019-07-02 21:47:10,376 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting 5 missing tasks from ResultStage 0 (MapPartitionsRDD[5] at collect at UDF.scala:39) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4))
2019-07-02 21:47:10,377 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 0.0 with 5 tasks
2019-07-02 21:47:10,417 [dispatcher-event-loop-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4827 bytes)
2019-07-02 21:47:10,440 [Executor task launch worker for task 0] INFO [org.apache.spark.executor.Executor] - Running task 0.0 in stage 0.0 (TID 0)
2019-07-02 21:47:10,527 [Executor task launch worker for task 0] INFO [org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator] - Code generated in 19.7792 ms
2019-07-02 21:47:10,560 [Executor task launch worker for task 0] INFO [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 0.0 (TID 0). 1117 bytes result sent to driver
2019-07-02 21:47:10,564 [dispatcher-event-loop-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 4833 bytes)
2019-07-02 21:47:10,564 [Executor task launch worker for task 1] INFO [org.apache.spark.executor.Executor] - Running task 1.0 in stage 0.0 (TID 1)
2019-07-02 21:47:10,608 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 0.0 (TID 0) in 204 ms on localhost (executor driver) (1/5)
2019-07-02 21:47:10,668 [Executor task launch worker for task 1] INFO [org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator] - Code generated in 29.6154 ms
2019-07-02 21:47:10,671 [Executor task launch worker for task 1] INFO [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 0.0 (TID 1). 1145 bytes result sent to driver
2019-07-02 21:47:10,672 [dispatcher-event-loop-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 2.0 in stage 0.0 (TID 2, localhost, executor driver, partition 2, PROCESS_LOCAL, 4835 bytes)
2019-07-02 21:47:10,672 [Executor task launch worker for task 2] INFO [org.apache.spark.executor.Executor] - Running task 2.0 in stage 0.0 (TID 2)
2019-07-02 21:47:10,674 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 0.0 (TID 1) in 111 ms on localhost (executor driver) (2/5)
2019-07-02 21:47:10,683 [Executor task launch worker for task 2] INFO [org.apache.spark.executor.Executor] - Finished task 2.0 in stage 0.0 (TID 2). 1102 bytes result sent to driver
2019-07-02 21:47:10,684 [dispatcher-event-loop-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 3.0 in stage 0.0 (TID 3, localhost, executor driver, partition 3, PROCESS_LOCAL, 4834 bytes)
2019-07-02 21:47:10,684 [Executor task launch worker for task 3] INFO [org.apache.spark.executor.Executor] - Running task 3.0 in stage 0.0 (TID 3)
2019-07-02 21:47:10,693 [Executor task launch worker for task 3] INFO [org.apache.spark.executor.Executor] - Finished task 3.0 in stage 0.0 (TID 3). 1102 bytes result sent to driver
2019-07-02 21:47:10,694 [dispatcher-event-loop-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 4.0 in stage 0.0 (TID 4, localhost, executor driver, partition 4, PROCESS_LOCAL, 4833 bytes)
2019-07-02 21:47:10,694 [Executor task launch worker for task 4] INFO [org.apache.spark.executor.Executor] - Running task 4.0 in stage 0.0 (TID 4)
2019-07-02 21:47:10,699 [task-result-getter-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 2.0 in stage 0.0 (TID 2) in 27 ms on localhost (executor driver) (3/5)
2019-07-02 21:47:10,701 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 3.0 in stage 0.0 (TID 3) in 17 ms on localhost (executor driver) (4/5)
2019-07-02 21:47:10,706 [Executor task launch worker for task 4] INFO [org.apache.spark.executor.Executor] - Finished task 4.0 in stage 0.0 (TID 4). 1102 bytes result sent to driver
2019-07-02 21:47:10,709 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 4.0 in stage 0.0 (TID 4) in 15 ms on localhost (executor driver) (5/5)
2019-07-02 21:47:10,710 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-07-02 21:47:10,711 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - ResultStage 0 (collect at UDF.scala:39) finished in 0.318 s
2019-07-02 21:47:10,747 [main] INFO [org.apache.spark.scheduler.DAGScheduler] - Job 0 finished: collect at UDF.scala:39, took 0.992153 s
2019-07-02 21:47:10,922 [main] INFO [org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator] - Code generated in 87.925 ms
2019-07-02 21:47:12,438 [Thread-1] INFO [org.apache.spark.SparkContext] - Invoking stop() from shutdown hook
2019-07-02 21:47:12,472 [Thread-1] INFO [org.spark_project.jetty.server.AbstractConnector] - Stopped Spark@f1a45f8{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-02 21:47:12,478 [Thread-1] INFO [org.apache.spark.ui.SparkUI] - Stopped Spark web UI at http://192.168.200.1:4040
2019-07-02 21:47:12,499 [dispatcher-event-loop-2] INFO [org.apache.spark.MapOutputTrackerMasterEndpoint] - MapOutputTrackerMasterEndpoint stopped!
2019-07-02 21:47:12,516 [Thread-1] INFO [org.apache.spark.storage.memory.MemoryStore] - MemoryStore cleared
2019-07-02 21:47:12,517 [Thread-1] INFO [org.apache.spark.storage.BlockManager] - BlockManager stopped
2019-07-02 21:47:12,519 [Thread-1] INFO [org.apache.spark.storage.BlockManagerMaster] - BlockManagerMaster stopped
2019-07-02 21:47:12,522 [dispatcher-event-loop-3] INFO [org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint] - OutputCommitCoordinator stopped!
2019-07-02 21:47:12,528 [Thread-1] INFO [org.apache.spark.SparkContext] - Successfully stopped SparkContext
2019-07-02 21:47:12,528 [Thread-1] INFO [org.apache.spark.util.ShutdownHookManager] - Shutdown hook called
2019-07-02 21:47:12,530 [Thread-1] INFO [org.apache.spark.util.ShutdownHookManager] - Deleting directory C:\Users\FrankCooper\AppData\Local\Temp\spark-c6c05433-fe03-474e-bd32-c0d2634fcdbe
