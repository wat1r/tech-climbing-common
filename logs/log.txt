2019-10-12 23:30:31,832 [main] INFO [org.apache.spark.SparkContext] - Running Spark version 2.2.0
2019-10-12 23:30:33,028 [main] WARN [org.apache.hadoop.util.NativeCodeLoader] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-10-12 23:30:35,359 [main] INFO [org.apache.spark.SparkContext] - Submitted application: RDDTwo
2019-10-12 23:30:35,396 [main] INFO [org.apache.spark.SecurityManager] - Changing view acls to: FrankCooper
2019-10-12 23:30:35,407 [main] INFO [org.apache.spark.SecurityManager] - Changing modify acls to: FrankCooper
2019-10-12 23:30:35,407 [main] INFO [org.apache.spark.SecurityManager] - Changing view acls groups to: 
2019-10-12 23:30:35,408 [main] INFO [org.apache.spark.SecurityManager] - Changing modify acls groups to: 
2019-10-12 23:30:35,409 [main] INFO [org.apache.spark.SecurityManager] - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(FrankCooper); groups with view permissions: Set(); users  with modify permissions: Set(FrankCooper); groups with modify permissions: Set()
2019-10-12 23:30:36,782 [main] INFO [org.apache.spark.util.Utils] - Successfully started service 'sparkDriver' on port 65281.
2019-10-12 23:30:36,823 [main] INFO [org.apache.spark.SparkEnv] - Registering MapOutputTracker
2019-10-12 23:30:36,858 [main] INFO [org.apache.spark.SparkEnv] - Registering BlockManagerMaster
2019-10-12 23:30:36,863 [main] INFO [org.apache.spark.storage.BlockManagerMasterEndpoint] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-10-12 23:30:36,864 [main] INFO [org.apache.spark.storage.BlockManagerMasterEndpoint] - BlockManagerMasterEndpoint up
2019-10-12 23:30:36,882 [main] INFO [org.apache.spark.storage.DiskBlockManager] - Created local directory at C:\Users\FrankCooper\AppData\Local\Temp\blockmgr-7e919582-eb6f-4703-aa54-09ab2f7a0c10
2019-10-12 23:30:36,957 [main] INFO [org.apache.spark.storage.memory.MemoryStore] - MemoryStore started with capacity 1989.6 MB
2019-10-12 23:30:37,057 [main] INFO [org.apache.spark.SparkEnv] - Registering OutputCommitCoordinator
2019-10-12 23:30:37,213 [main] INFO [org.spark_project.jetty.util.log] - Logging initialized @7483ms
2019-10-12 23:30:37,337 [main] INFO [org.spark_project.jetty.server.Server] - jetty-9.3.z-SNAPSHOT
2019-10-12 23:30:37,352 [main] INFO [org.spark_project.jetty.server.Server] - Started @7621ms
2019-10-12 23:30:37,378 [main] INFO [org.spark_project.jetty.server.AbstractConnector] - Started ServerConnector@34cf294c{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-10-12 23:30:37,378 [main] INFO [org.apache.spark.util.Utils] - Successfully started service 'SparkUI' on port 4040.
2019-10-12 23:30:37,437 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@253b380a{/jobs,null,AVAILABLE,@Spark}
2019-10-12 23:30:37,439 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@34a0ef00{/jobs/json,null,AVAILABLE,@Spark}
2019-10-12 23:30:37,440 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@4f8b4bd0{/jobs/job,null,AVAILABLE,@Spark}
2019-10-12 23:30:37,441 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@597f48df{/jobs/job/json,null,AVAILABLE,@Spark}
2019-10-12 23:30:37,442 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1d96d872{/stages,null,AVAILABLE,@Spark}
2019-10-12 23:30:37,443 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@234a8f27{/stages/json,null,AVAILABLE,@Spark}
2019-10-12 23:30:37,444 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@58d63b16{/stages/stage,null,AVAILABLE,@Spark}
2019-10-12 23:30:37,446 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@46866946{/stages/stage/json,null,AVAILABLE,@Spark}
2019-10-12 23:30:37,448 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@42b21d99{/stages/pool,null,AVAILABLE,@Spark}
2019-10-12 23:30:37,449 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@37b72ea{/stages/pool/json,null,AVAILABLE,@Spark}
2019-10-12 23:30:37,450 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@47547132{/storage,null,AVAILABLE,@Spark}
2019-10-12 23:30:37,451 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@5e1218b4{/storage/json,null,AVAILABLE,@Spark}
2019-10-12 23:30:37,453 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1f966492{/storage/rdd,null,AVAILABLE,@Spark}
2019-10-12 23:30:37,454 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@4a68135e{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-10-12 23:30:37,455 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@6a0ac48e{/environment,null,AVAILABLE,@Spark}
2019-10-12 23:30:37,456 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@35636217{/environment/json,null,AVAILABLE,@Spark}
2019-10-12 23:30:37,456 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@4647881c{/executors,null,AVAILABLE,@Spark}
2019-10-12 23:30:37,458 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@10667848{/executors/json,null,AVAILABLE,@Spark}
2019-10-12 23:30:37,459 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@2c6ee758{/executors/threadDump,null,AVAILABLE,@Spark}
2019-10-12 23:30:37,460 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@203c20cf{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-10-12 23:30:37,474 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@2116b68b{/static,null,AVAILABLE,@Spark}
2019-10-12 23:30:37,476 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@6fd5717c{/,null,AVAILABLE,@Spark}
2019-10-12 23:30:37,477 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@3fdecce{/api,null,AVAILABLE,@Spark}
2019-10-12 23:30:37,478 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@5cbd159f{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-10-12 23:30:37,479 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@67b7c170{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-10-12 23:30:37,483 [main] INFO [org.apache.spark.ui.SparkUI] - Bound SparkUI to 0.0.0.0, and started at http://192.168.200.1:4040
2019-10-12 23:30:37,656 [main] INFO [org.apache.spark.executor.Executor] - Starting executor ID driver on host localhost
2019-10-12 23:30:37,697 [main] INFO [org.apache.spark.util.Utils] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 65290.
2019-10-12 23:30:37,698 [main] INFO [org.apache.spark.network.netty.NettyBlockTransferService] - Server created on 192.168.200.1:65290
2019-10-12 23:30:37,699 [main] INFO [org.apache.spark.storage.BlockManager] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-10-12 23:30:37,702 [main] INFO [org.apache.spark.storage.BlockManagerMaster] - Registering BlockManager BlockManagerId(driver, 192.168.200.1, 65290, None)
2019-10-12 23:30:37,706 [dispatcher-event-loop-2] INFO [org.apache.spark.storage.BlockManagerMasterEndpoint] - Registering block manager 192.168.200.1:65290 with 1989.6 MB RAM, BlockManagerId(driver, 192.168.200.1, 65290, None)
2019-10-12 23:30:37,709 [main] INFO [org.apache.spark.storage.BlockManagerMaster] - Registered BlockManager BlockManagerId(driver, 192.168.200.1, 65290, None)
2019-10-12 23:30:37,712 [main] INFO [org.apache.spark.storage.BlockManager] - Initialized BlockManager: BlockManagerId(driver, 192.168.200.1, 65290, None)
2019-10-12 23:30:37,946 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@74174a23{/metrics/json,null,AVAILABLE,@Spark}
2019-10-12 23:30:38,235 [main] INFO [org.apache.spark.sql.internal.SharedState] - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/E:/StudyingCourse/IntellijModule/tech-climbing-common/spark-warehouse/').
2019-10-12 23:30:38,243 [main] INFO [org.apache.spark.sql.internal.SharedState] - Warehouse path is 'file:/E:/StudyingCourse/IntellijModule/tech-climbing-common/spark-warehouse/'.
2019-10-12 23:30:38,250 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@43fda8d9{/SQL,null,AVAILABLE,@Spark}
2019-10-12 23:30:38,251 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@6a87026{/SQL/json,null,AVAILABLE,@Spark}
2019-10-12 23:30:38,253 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@4288d98e{/SQL/execution,null,AVAILABLE,@Spark}
2019-10-12 23:30:38,253 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@7c601d50{/SQL/execution/json,null,AVAILABLE,@Spark}
2019-10-12 23:30:38,256 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@50f097b5{/static/sql,null,AVAILABLE,@Spark}
2019-10-12 23:30:39,144 [main] INFO [org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef] - Registered StateStoreCoordinator endpoint
2019-10-12 23:30:58,187 [main] INFO [org.apache.spark.SparkContext] - Starting job: foreach at RDDTwo.scala:32
2019-10-12 23:30:58,230 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Got job 0 (foreach at RDDTwo.scala:32) with 2 output partitions
2019-10-12 23:30:58,230 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 0 (foreach at RDDTwo.scala:32)
2019-10-12 23:30:58,230 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2019-10-12 23:30:58,233 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2019-10-12 23:30:58,241 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 0 (MapPartitionsRDD[1] at mapPartitions at RDDTwo.scala:26), which has no missing parents
2019-10-12 23:30:58,578 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_0 stored as values in memory (estimated size 1944.0 B, free 1989.6 MB)
2019-10-12 23:30:59,997 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 1273.0 B, free 1989.6 MB)
2019-10-12 23:31:00,000 [dispatcher-event-loop-3] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_0_piece0 in memory on 192.168.200.1:65290 (size: 1273.0 B, free: 1989.6 MB)
2019-10-12 23:31:00,006 [dag-scheduler-event-loop] INFO [org.apache.spark.SparkContext] - Created broadcast 0 from broadcast at DAGScheduler.scala:1006
2019-10-12 23:31:00,042 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at mapPartitions at RDDTwo.scala:26) (first 15 tasks are for partitions Vector(0, 1))
2019-10-12 23:31:00,043 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 0.0 with 2 tasks
2019-10-12 23:31:00,087 [dispatcher-event-loop-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4764 bytes)
2019-10-12 23:31:00,104 [Executor task launch worker for task 0] INFO [org.apache.spark.executor.Executor] - Running task 0.0 in stage 0.0 (TID 0)
2019-10-12 23:31:00,212 [Executor task launch worker for task 0] INFO [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 0.0 (TID 0). 708 bytes result sent to driver
2019-10-12 23:31:00,215 [dispatcher-event-loop-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 4764 bytes)
2019-10-12 23:31:00,215 [Executor task launch worker for task 1] INFO [org.apache.spark.executor.Executor] - Running task 1.0 in stage 0.0 (TID 1)
2019-10-12 23:31:00,225 [Executor task launch worker for task 1] INFO [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 0.0 (TID 1). 708 bytes result sent to driver
2019-10-12 23:31:00,240 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 0.0 (TID 0) in 169 ms on localhost (executor driver) (1/2)
2019-10-12 23:31:00,242 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 0.0 (TID 1) in 28 ms on localhost (executor driver) (2/2)
2019-10-12 23:31:00,243 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-10-12 23:31:00,258 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - ResultStage 0 (foreach at RDDTwo.scala:32) finished in 0.189 s
2019-10-12 23:31:00,271 [main] INFO [org.apache.spark.scheduler.DAGScheduler] - Job 0 finished: foreach at RDDTwo.scala:32, took 2.084818 s
2019-10-12 23:31:00,276 [Thread-1] INFO [org.apache.spark.SparkContext] - Invoking stop() from shutdown hook
2019-10-12 23:31:00,285 [Thread-1] INFO [org.spark_project.jetty.server.AbstractConnector] - Stopped Spark@34cf294c{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-10-12 23:31:00,291 [Thread-1] INFO [org.apache.spark.ui.SparkUI] - Stopped Spark web UI at http://192.168.200.1:4040
2019-10-12 23:31:00,361 [dispatcher-event-loop-3] INFO [org.apache.spark.MapOutputTrackerMasterEndpoint] - MapOutputTrackerMasterEndpoint stopped!
2019-10-12 23:31:00,391 [Thread-1] INFO [org.apache.spark.storage.memory.MemoryStore] - MemoryStore cleared
2019-10-12 23:31:00,391 [Thread-1] INFO [org.apache.spark.storage.BlockManager] - BlockManager stopped
2019-10-12 23:31:00,394 [Thread-1] INFO [org.apache.spark.storage.BlockManagerMaster] - BlockManagerMaster stopped
2019-10-12 23:31:00,408 [dispatcher-event-loop-0] INFO [org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint] - OutputCommitCoordinator stopped!
2019-10-12 23:31:00,411 [Thread-1] INFO [org.apache.spark.SparkContext] - Successfully stopped SparkContext
2019-10-12 23:31:00,411 [Thread-1] INFO [org.apache.spark.util.ShutdownHookManager] - Shutdown hook called
2019-10-12 23:31:00,412 [Thread-1] INFO [org.apache.spark.util.ShutdownHookManager] - Deleting directory C:\Users\FrankCooper\AppData\Local\Temp\spark-35d4f9f9-67c3-43cd-b3ec-35df3383aa65
2019-10-12 23:33:39,447 [main] INFO [org.apache.spark.SparkContext] - Running Spark version 2.2.0
2019-10-12 23:33:39,782 [main] WARN [org.apache.hadoop.util.NativeCodeLoader] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-10-12 23:33:40,027 [main] INFO [org.apache.spark.SparkContext] - Submitted application: RDDTwo
2019-10-12 23:33:40,044 [main] INFO [org.apache.spark.SecurityManager] - Changing view acls to: FrankCooper
2019-10-12 23:33:40,045 [main] INFO [org.apache.spark.SecurityManager] - Changing modify acls to: FrankCooper
2019-10-12 23:33:40,045 [main] INFO [org.apache.spark.SecurityManager] - Changing view acls groups to: 
2019-10-12 23:33:40,046 [main] INFO [org.apache.spark.SecurityManager] - Changing modify acls groups to: 
2019-10-12 23:33:40,047 [main] INFO [org.apache.spark.SecurityManager] - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(FrankCooper); groups with view permissions: Set(); users  with modify permissions: Set(FrankCooper); groups with modify permissions: Set()
2019-10-12 23:33:40,559 [main] INFO [org.apache.spark.util.Utils] - Successfully started service 'sparkDriver' on port 65342.
2019-10-12 23:33:40,576 [main] INFO [org.apache.spark.SparkEnv] - Registering MapOutputTracker
2019-10-12 23:33:40,591 [main] INFO [org.apache.spark.SparkEnv] - Registering BlockManagerMaster
2019-10-12 23:33:40,595 [main] INFO [org.apache.spark.storage.BlockManagerMasterEndpoint] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-10-12 23:33:40,595 [main] INFO [org.apache.spark.storage.BlockManagerMasterEndpoint] - BlockManagerMasterEndpoint up
2019-10-12 23:33:40,605 [main] INFO [org.apache.spark.storage.DiskBlockManager] - Created local directory at C:\Users\FrankCooper\AppData\Local\Temp\blockmgr-cf88952f-68c5-4505-9155-41f952797b65
2019-10-12 23:33:40,657 [main] INFO [org.apache.spark.storage.memory.MemoryStore] - MemoryStore started with capacity 1989.6 MB
2019-10-12 23:33:40,702 [main] INFO [org.apache.spark.SparkEnv] - Registering OutputCommitCoordinator
2019-10-12 23:33:40,789 [main] INFO [org.spark_project.jetty.util.log] - Logging initialized @3114ms
2019-10-12 23:33:40,850 [main] INFO [org.spark_project.jetty.server.Server] - jetty-9.3.z-SNAPSHOT
2019-10-12 23:33:40,863 [main] INFO [org.spark_project.jetty.server.Server] - Started @3187ms
2019-10-12 23:33:40,882 [main] INFO [org.spark_project.jetty.server.AbstractConnector] - Started ServerConnector@34cf294c{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-10-12 23:33:40,882 [main] INFO [org.apache.spark.util.Utils] - Successfully started service 'SparkUI' on port 4040.
2019-10-12 23:33:40,905 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@253b380a{/jobs,null,AVAILABLE,@Spark}
2019-10-12 23:33:40,906 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@34a0ef00{/jobs/json,null,AVAILABLE,@Spark}
2019-10-12 23:33:40,907 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@4f8b4bd0{/jobs/job,null,AVAILABLE,@Spark}
2019-10-12 23:33:40,908 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@597f48df{/jobs/job/json,null,AVAILABLE,@Spark}
2019-10-12 23:33:40,909 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1d96d872{/stages,null,AVAILABLE,@Spark}
2019-10-12 23:33:40,910 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@234a8f27{/stages/json,null,AVAILABLE,@Spark}
2019-10-12 23:33:40,912 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@58d63b16{/stages/stage,null,AVAILABLE,@Spark}
2019-10-12 23:33:40,913 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@46866946{/stages/stage/json,null,AVAILABLE,@Spark}
2019-10-12 23:33:40,915 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@42b21d99{/stages/pool,null,AVAILABLE,@Spark}
2019-10-12 23:33:40,916 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@37b72ea{/stages/pool/json,null,AVAILABLE,@Spark}
2019-10-12 23:33:40,917 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@47547132{/storage,null,AVAILABLE,@Spark}
2019-10-12 23:33:40,919 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@5e1218b4{/storage/json,null,AVAILABLE,@Spark}
2019-10-12 23:33:40,920 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1f966492{/storage/rdd,null,AVAILABLE,@Spark}
2019-10-12 23:33:40,922 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@4a68135e{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-10-12 23:33:40,923 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@6a0ac48e{/environment,null,AVAILABLE,@Spark}
2019-10-12 23:33:40,924 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@35636217{/environment/json,null,AVAILABLE,@Spark}
2019-10-12 23:33:40,926 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@4647881c{/executors,null,AVAILABLE,@Spark}
2019-10-12 23:33:40,927 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@10667848{/executors/json,null,AVAILABLE,@Spark}
2019-10-12 23:33:40,928 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@2c6ee758{/executors/threadDump,null,AVAILABLE,@Spark}
2019-10-12 23:33:40,930 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@203c20cf{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-10-12 23:33:40,935 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@2116b68b{/static,null,AVAILABLE,@Spark}
2019-10-12 23:33:40,937 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@6fd5717c{/,null,AVAILABLE,@Spark}
2019-10-12 23:33:40,938 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@3fdecce{/api,null,AVAILABLE,@Spark}
2019-10-12 23:33:40,939 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@5cbd159f{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-10-12 23:33:40,940 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@67b7c170{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-10-12 23:33:40,942 [main] INFO [org.apache.spark.ui.SparkUI] - Bound SparkUI to 0.0.0.0, and started at http://192.168.200.1:4040
2019-10-12 23:33:41,000 [main] INFO [org.apache.spark.executor.Executor] - Starting executor ID driver on host localhost
2019-10-12 23:33:41,018 [main] INFO [org.apache.spark.util.Utils] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 65351.
2019-10-12 23:33:41,018 [main] INFO [org.apache.spark.network.netty.NettyBlockTransferService] - Server created on 192.168.200.1:65351
2019-10-12 23:33:41,019 [main] INFO [org.apache.spark.storage.BlockManager] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-10-12 23:33:41,020 [main] INFO [org.apache.spark.storage.BlockManagerMaster] - Registering BlockManager BlockManagerId(driver, 192.168.200.1, 65351, None)
2019-10-12 23:33:41,022 [dispatcher-event-loop-2] INFO [org.apache.spark.storage.BlockManagerMasterEndpoint] - Registering block manager 192.168.200.1:65351 with 1989.6 MB RAM, BlockManagerId(driver, 192.168.200.1, 65351, None)
2019-10-12 23:33:41,024 [main] INFO [org.apache.spark.storage.BlockManagerMaster] - Registered BlockManager BlockManagerId(driver, 192.168.200.1, 65351, None)
2019-10-12 23:33:41,025 [main] INFO [org.apache.spark.storage.BlockManager] - Initialized BlockManager: BlockManagerId(driver, 192.168.200.1, 65351, None)
2019-10-12 23:33:41,182 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@74174a23{/metrics/json,null,AVAILABLE,@Spark}
2019-10-12 23:33:41,257 [main] INFO [org.apache.spark.sql.internal.SharedState] - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/E:/StudyingCourse/IntellijModule/tech-climbing-common/spark-warehouse/').
2019-10-12 23:33:41,258 [main] INFO [org.apache.spark.sql.internal.SharedState] - Warehouse path is 'file:/E:/StudyingCourse/IntellijModule/tech-climbing-common/spark-warehouse/'.
2019-10-12 23:33:41,266 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@43fda8d9{/SQL,null,AVAILABLE,@Spark}
2019-10-12 23:33:41,267 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@6a87026{/SQL/json,null,AVAILABLE,@Spark}
2019-10-12 23:33:41,269 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@4288d98e{/SQL/execution,null,AVAILABLE,@Spark}
2019-10-12 23:33:41,270 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@7c601d50{/SQL/execution/json,null,AVAILABLE,@Spark}
2019-10-12 23:33:41,272 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@50f097b5{/static/sql,null,AVAILABLE,@Spark}
2019-10-12 23:33:41,972 [main] INFO [org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef] - Registered StateStoreCoordinator endpoint
2019-10-12 23:33:42,200 [main] INFO [org.apache.spark.SparkContext] - Starting job: foreach at RDDTwo.scala:45
2019-10-12 23:33:42,218 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Got job 0 (foreach at RDDTwo.scala:45) with 1 output partitions
2019-10-12 23:33:42,218 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 0 (foreach at RDDTwo.scala:45)
2019-10-12 23:33:42,219 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2019-10-12 23:33:42,220 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2019-10-12 23:33:42,225 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 0 (MapPartitionsRDD[1] at mapPartitionsWithIndex at RDDTwo.scala:38), which has no missing parents
2019-10-12 23:33:42,331 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_0 stored as values in memory (estimated size 1992.0 B, free 1989.6 MB)
2019-10-12 23:33:42,380 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 1281.0 B, free 1989.6 MB)
2019-10-12 23:33:42,383 [dispatcher-event-loop-1] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_0_piece0 in memory on 192.168.200.1:65351 (size: 1281.0 B, free: 1989.6 MB)
2019-10-12 23:33:42,385 [dag-scheduler-event-loop] INFO [org.apache.spark.SparkContext] - Created broadcast 0 from broadcast at DAGScheduler.scala:1006
2019-10-12 23:33:42,402 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at mapPartitionsWithIndex at RDDTwo.scala:38) (first 15 tasks are for partitions Vector(0))
2019-10-12 23:33:42,402 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 0.0 with 1 tasks
2019-10-12 23:33:42,455 [dispatcher-event-loop-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4784 bytes)
2019-10-12 23:33:42,461 [Executor task launch worker for task 0] INFO [org.apache.spark.executor.Executor] - Running task 0.0 in stage 0.0 (TID 0)
2019-10-12 23:33:42,538 [Executor task launch worker for task 0] INFO [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 0.0 (TID 0). 751 bytes result sent to driver
2019-10-12 23:33:42,547 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 0.0 (TID 0) in 108 ms on localhost (executor driver) (1/1)
2019-10-12 23:33:42,549 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-10-12 23:33:42,553 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - ResultStage 0 (foreach at RDDTwo.scala:45) finished in 0.131 s
2019-10-12 23:33:42,560 [main] INFO [org.apache.spark.scheduler.DAGScheduler] - Job 0 finished: foreach at RDDTwo.scala:45, took 0.359471 s
2019-10-12 23:33:42,565 [Thread-1] INFO [org.apache.spark.SparkContext] - Invoking stop() from shutdown hook
2019-10-12 23:33:42,573 [Thread-1] INFO [org.spark_project.jetty.server.AbstractConnector] - Stopped Spark@34cf294c{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-10-12 23:33:42,576 [Thread-1] INFO [org.apache.spark.ui.SparkUI] - Stopped Spark web UI at http://192.168.200.1:4040
2019-10-12 23:33:42,589 [dispatcher-event-loop-3] INFO [org.apache.spark.MapOutputTrackerMasterEndpoint] - MapOutputTrackerMasterEndpoint stopped!
2019-10-12 23:33:42,597 [Thread-1] INFO [org.apache.spark.storage.memory.MemoryStore] - MemoryStore cleared
2019-10-12 23:33:42,598 [Thread-1] INFO [org.apache.spark.storage.BlockManager] - BlockManager stopped
2019-10-12 23:33:42,602 [Thread-1] INFO [org.apache.spark.storage.BlockManagerMaster] - BlockManagerMaster stopped
2019-10-12 23:33:42,604 [dispatcher-event-loop-3] INFO [org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint] - OutputCommitCoordinator stopped!
2019-10-12 23:33:42,606 [Thread-1] INFO [org.apache.spark.SparkContext] - Successfully stopped SparkContext
2019-10-12 23:33:42,606 [Thread-1] INFO [org.apache.spark.util.ShutdownHookManager] - Shutdown hook called
2019-10-12 23:33:42,607 [Thread-1] INFO [org.apache.spark.util.ShutdownHookManager] - Deleting directory C:\Users\FrankCooper\AppData\Local\Temp\spark-17fc1a1c-f883-494c-a906-e05d8c2c6388
2019-10-12 23:36:37,687 [main] INFO [org.apache.spark.SparkContext] - Running Spark version 2.2.0
2019-10-12 23:36:38,145 [main] WARN [org.apache.hadoop.util.NativeCodeLoader] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-10-12 23:36:38,325 [main] INFO [org.apache.spark.SparkContext] - Submitted application: RDDTwo
2019-10-12 23:36:38,341 [main] INFO [org.apache.spark.SecurityManager] - Changing view acls to: FrankCooper
2019-10-12 23:36:38,342 [main] INFO [org.apache.spark.SecurityManager] - Changing modify acls to: FrankCooper
2019-10-12 23:36:38,342 [main] INFO [org.apache.spark.SecurityManager] - Changing view acls groups to: 
2019-10-12 23:36:38,343 [main] INFO [org.apache.spark.SecurityManager] - Changing modify acls groups to: 
2019-10-12 23:36:38,343 [main] INFO [org.apache.spark.SecurityManager] - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(FrankCooper); groups with view permissions: Set(); users  with modify permissions: Set(FrankCooper); groups with modify permissions: Set()
2019-10-12 23:36:38,873 [main] INFO [org.apache.spark.util.Utils] - Successfully started service 'sparkDriver' on port 65403.
2019-10-12 23:36:38,890 [main] INFO [org.apache.spark.SparkEnv] - Registering MapOutputTracker
2019-10-12 23:36:38,907 [main] INFO [org.apache.spark.SparkEnv] - Registering BlockManagerMaster
2019-10-12 23:36:38,910 [main] INFO [org.apache.spark.storage.BlockManagerMasterEndpoint] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-10-12 23:36:38,910 [main] INFO [org.apache.spark.storage.BlockManagerMasterEndpoint] - BlockManagerMasterEndpoint up
2019-10-12 23:36:38,919 [main] INFO [org.apache.spark.storage.DiskBlockManager] - Created local directory at C:\Users\FrankCooper\AppData\Local\Temp\blockmgr-71a1336c-af01-41cf-91e2-b27ac5e451c7
2019-10-12 23:36:38,965 [main] INFO [org.apache.spark.storage.memory.MemoryStore] - MemoryStore started with capacity 1989.6 MB
2019-10-12 23:36:39,004 [main] INFO [org.apache.spark.SparkEnv] - Registering OutputCommitCoordinator
2019-10-12 23:36:39,668 [main] INFO [org.spark_project.jetty.util.log] - Logging initialized @3602ms
2019-10-12 23:36:39,723 [main] INFO [org.spark_project.jetty.server.Server] - jetty-9.3.z-SNAPSHOT
2019-10-12 23:36:39,734 [main] INFO [org.spark_project.jetty.server.Server] - Started @3668ms
2019-10-12 23:36:39,756 [main] INFO [org.spark_project.jetty.server.AbstractConnector] - Started ServerConnector@34cf294c{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-10-12 23:36:39,756 [main] INFO [org.apache.spark.util.Utils] - Successfully started service 'SparkUI' on port 4040.
2019-10-12 23:36:39,783 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@253b380a{/jobs,null,AVAILABLE,@Spark}
2019-10-12 23:36:39,786 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@34a0ef00{/jobs/json,null,AVAILABLE,@Spark}
2019-10-12 23:36:39,787 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@4f8b4bd0{/jobs/job,null,AVAILABLE,@Spark}
2019-10-12 23:36:39,791 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@597f48df{/jobs/job/json,null,AVAILABLE,@Spark}
2019-10-12 23:36:39,794 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1d96d872{/stages,null,AVAILABLE,@Spark}
2019-10-12 23:36:39,797 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@234a8f27{/stages/json,null,AVAILABLE,@Spark}
2019-10-12 23:36:39,798 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@58d63b16{/stages/stage,null,AVAILABLE,@Spark}
2019-10-12 23:36:39,800 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@46866946{/stages/stage/json,null,AVAILABLE,@Spark}
2019-10-12 23:36:39,802 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@42b21d99{/stages/pool,null,AVAILABLE,@Spark}
2019-10-12 23:36:39,803 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@37b72ea{/stages/pool/json,null,AVAILABLE,@Spark}
2019-10-12 23:36:39,804 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@47547132{/storage,null,AVAILABLE,@Spark}
2019-10-12 23:36:39,805 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@5e1218b4{/storage/json,null,AVAILABLE,@Spark}
2019-10-12 23:36:39,806 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1f966492{/storage/rdd,null,AVAILABLE,@Spark}
2019-10-12 23:36:39,808 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@4a68135e{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-10-12 23:36:39,812 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@6a0ac48e{/environment,null,AVAILABLE,@Spark}
2019-10-12 23:36:39,813 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@35636217{/environment/json,null,AVAILABLE,@Spark}
2019-10-12 23:36:39,814 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@4647881c{/executors,null,AVAILABLE,@Spark}
2019-10-12 23:36:39,815 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@10667848{/executors/json,null,AVAILABLE,@Spark}
2019-10-12 23:36:39,817 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@2c6ee758{/executors/threadDump,null,AVAILABLE,@Spark}
2019-10-12 23:36:39,818 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@203c20cf{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-10-12 23:36:39,825 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@2116b68b{/static,null,AVAILABLE,@Spark}
2019-10-12 23:36:39,827 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@6fd5717c{/,null,AVAILABLE,@Spark}
2019-10-12 23:36:39,831 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@3fdecce{/api,null,AVAILABLE,@Spark}
2019-10-12 23:36:39,832 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@5cbd159f{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-10-12 23:36:39,834 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@67b7c170{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-10-12 23:36:39,836 [main] INFO [org.apache.spark.ui.SparkUI] - Bound SparkUI to 0.0.0.0, and started at http://192.168.200.1:4040
2019-10-12 23:36:39,911 [main] INFO [org.apache.spark.executor.Executor] - Starting executor ID driver on host localhost
2019-10-12 23:36:39,930 [main] INFO [org.apache.spark.util.Utils] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 65412.
2019-10-12 23:36:39,931 [main] INFO [org.apache.spark.network.netty.NettyBlockTransferService] - Server created on 192.168.200.1:65412
2019-10-12 23:36:39,932 [main] INFO [org.apache.spark.storage.BlockManager] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-10-12 23:36:39,933 [main] INFO [org.apache.spark.storage.BlockManagerMaster] - Registering BlockManager BlockManagerId(driver, 192.168.200.1, 65412, None)
2019-10-12 23:36:39,936 [dispatcher-event-loop-2] INFO [org.apache.spark.storage.BlockManagerMasterEndpoint] - Registering block manager 192.168.200.1:65412 with 1989.6 MB RAM, BlockManagerId(driver, 192.168.200.1, 65412, None)
2019-10-12 23:36:39,938 [main] INFO [org.apache.spark.storage.BlockManagerMaster] - Registered BlockManager BlockManagerId(driver, 192.168.200.1, 65412, None)
2019-10-12 23:36:39,939 [main] INFO [org.apache.spark.storage.BlockManager] - Initialized BlockManager: BlockManagerId(driver, 192.168.200.1, 65412, None)
2019-10-12 23:36:40,070 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@74174a23{/metrics/json,null,AVAILABLE,@Spark}
2019-10-12 23:36:40,133 [main] INFO [org.apache.spark.sql.internal.SharedState] - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/E:/StudyingCourse/IntellijModule/tech-climbing-common/spark-warehouse/').
2019-10-12 23:36:40,133 [main] INFO [org.apache.spark.sql.internal.SharedState] - Warehouse path is 'file:/E:/StudyingCourse/IntellijModule/tech-climbing-common/spark-warehouse/'.
2019-10-12 23:36:40,139 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@43fda8d9{/SQL,null,AVAILABLE,@Spark}
2019-10-12 23:36:40,140 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@6a87026{/SQL/json,null,AVAILABLE,@Spark}
2019-10-12 23:36:40,141 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@4288d98e{/SQL/execution,null,AVAILABLE,@Spark}
2019-10-12 23:36:40,142 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@7c601d50{/SQL/execution/json,null,AVAILABLE,@Spark}
2019-10-12 23:36:40,143 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@50f097b5{/static/sql,null,AVAILABLE,@Spark}
2019-10-12 23:36:40,708 [main] INFO [org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef] - Registered StateStoreCoordinator endpoint
2019-10-12 23:36:40,977 [main] INFO [org.apache.spark.SparkContext] - Starting job: reduce at RDDTwo.scala:52
2019-10-12 23:36:40,993 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Got job 0 (reduce at RDDTwo.scala:52) with 1 output partitions
2019-10-12 23:36:40,993 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 0 (reduce at RDDTwo.scala:52)
2019-10-12 23:36:40,994 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2019-10-12 23:36:40,995 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2019-10-12 23:36:40,999 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 0 (ParallelCollectionRDD[0] at parallelize at RDDTwo.scala:51), which has no missing parents
2019-10-12 23:36:41,094 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_0 stored as values in memory (estimated size 1312.0 B, free 1989.6 MB)
2019-10-12 23:36:41,143 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 918.0 B, free 1989.6 MB)
2019-10-12 23:36:41,145 [dispatcher-event-loop-1] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_0_piece0 in memory on 192.168.200.1:65412 (size: 918.0 B, free: 1989.6 MB)
2019-10-12 23:36:41,147 [dag-scheduler-event-loop] INFO [org.apache.spark.SparkContext] - Created broadcast 0 from broadcast at DAGScheduler.scala:1006
2019-10-12 23:36:41,162 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting 1 missing tasks from ResultStage 0 (ParallelCollectionRDD[0] at parallelize at RDDTwo.scala:51) (first 15 tasks are for partitions Vector(0))
2019-10-12 23:36:41,163 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 0.0 with 1 tasks
2019-10-12 23:36:41,204 [dispatcher-event-loop-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4776 bytes)
2019-10-12 23:36:41,210 [Executor task launch worker for task 0] INFO [org.apache.spark.executor.Executor] - Running task 0.0 in stage 0.0 (TID 0)
2019-10-12 23:36:41,384 [Executor task launch worker for task 0] INFO [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 0.0 (TID 0). 824 bytes result sent to driver
2019-10-12 23:36:41,392 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 0.0 (TID 0) in 206 ms on localhost (executor driver) (1/1)
2019-10-12 23:36:41,394 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-10-12 23:36:41,397 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - ResultStage 0 (reduce at RDDTwo.scala:52) finished in 0.222 s
2019-10-12 23:36:41,403 [main] INFO [org.apache.spark.scheduler.DAGScheduler] - Job 0 finished: reduce at RDDTwo.scala:52, took 0.425673 s
2019-10-12 23:36:41,428 [Thread-1] INFO [org.apache.spark.SparkContext] - Invoking stop() from shutdown hook
2019-10-12 23:36:41,435 [Thread-1] INFO [org.spark_project.jetty.server.AbstractConnector] - Stopped Spark@34cf294c{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-10-12 23:36:41,438 [Thread-1] INFO [org.apache.spark.ui.SparkUI] - Stopped Spark web UI at http://192.168.200.1:4040
2019-10-12 23:36:41,458 [dispatcher-event-loop-3] INFO [org.apache.spark.MapOutputTrackerMasterEndpoint] - MapOutputTrackerMasterEndpoint stopped!
2019-10-12 23:36:41,471 [Thread-1] INFO [org.apache.spark.storage.memory.MemoryStore] - MemoryStore cleared
2019-10-12 23:36:41,472 [Thread-1] INFO [org.apache.spark.storage.BlockManager] - BlockManager stopped
2019-10-12 23:36:41,476 [Thread-1] INFO [org.apache.spark.storage.BlockManagerMaster] - BlockManagerMaster stopped
2019-10-12 23:36:41,479 [dispatcher-event-loop-0] INFO [org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint] - OutputCommitCoordinator stopped!
2019-10-12 23:36:41,481 [Thread-1] INFO [org.apache.spark.SparkContext] - Successfully stopped SparkContext
2019-10-12 23:36:41,482 [Thread-1] INFO [org.apache.spark.util.ShutdownHookManager] - Shutdown hook called
2019-10-12 23:36:41,483 [Thread-1] INFO [org.apache.spark.util.ShutdownHookManager] - Deleting directory C:\Users\FrankCooper\AppData\Local\Temp\spark-240bfeb5-7f3d-4336-802e-d540bf4625da
2019-10-12 23:37:34,309 [main] INFO [org.apache.spark.SparkContext] - Running Spark version 2.2.0
2019-10-12 23:37:34,631 [main] WARN [org.apache.hadoop.util.NativeCodeLoader] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-10-12 23:37:34,777 [main] INFO [org.apache.spark.SparkContext] - Submitted application: RDDTwo
2019-10-12 23:37:34,794 [main] INFO [org.apache.spark.SecurityManager] - Changing view acls to: FrankCooper
2019-10-12 23:37:34,795 [main] INFO [org.apache.spark.SecurityManager] - Changing modify acls to: FrankCooper
2019-10-12 23:37:34,795 [main] INFO [org.apache.spark.SecurityManager] - Changing view acls groups to: 
2019-10-12 23:37:34,795 [main] INFO [org.apache.spark.SecurityManager] - Changing modify acls groups to: 
2019-10-12 23:37:34,796 [main] INFO [org.apache.spark.SecurityManager] - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(FrankCooper); groups with view permissions: Set(); users  with modify permissions: Set(FrankCooper); groups with modify permissions: Set()
2019-10-12 23:37:35,325 [main] INFO [org.apache.spark.util.Utils] - Successfully started service 'sparkDriver' on port 65441.
2019-10-12 23:37:35,343 [main] INFO [org.apache.spark.SparkEnv] - Registering MapOutputTracker
2019-10-12 23:37:35,358 [main] INFO [org.apache.spark.SparkEnv] - Registering BlockManagerMaster
2019-10-12 23:37:35,362 [main] INFO [org.apache.spark.storage.BlockManagerMasterEndpoint] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-10-12 23:37:35,362 [main] INFO [org.apache.spark.storage.BlockManagerMasterEndpoint] - BlockManagerMasterEndpoint up
2019-10-12 23:37:35,371 [main] INFO [org.apache.spark.storage.DiskBlockManager] - Created local directory at C:\Users\FrankCooper\AppData\Local\Temp\blockmgr-0382646f-50a4-432d-850b-3a36d85c24df
2019-10-12 23:37:35,418 [main] INFO [org.apache.spark.storage.memory.MemoryStore] - MemoryStore started with capacity 1989.6 MB
2019-10-12 23:37:35,471 [main] INFO [org.apache.spark.SparkEnv] - Registering OutputCommitCoordinator
2019-10-12 23:37:35,543 [main] INFO [org.spark_project.jetty.util.log] - Logging initialized @3273ms
2019-10-12 23:37:35,608 [main] INFO [org.spark_project.jetty.server.Server] - jetty-9.3.z-SNAPSHOT
2019-10-12 23:37:35,620 [main] INFO [org.spark_project.jetty.server.Server] - Started @3351ms
2019-10-12 23:37:35,643 [main] INFO [org.spark_project.jetty.server.AbstractConnector] - Started ServerConnector@51351f28{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-10-12 23:37:35,643 [main] INFO [org.apache.spark.util.Utils] - Successfully started service 'SparkUI' on port 4040.
2019-10-12 23:37:35,670 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@42f9c19a{/jobs,null,AVAILABLE,@Spark}
2019-10-12 23:37:35,671 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1a3e5f23{/jobs/json,null,AVAILABLE,@Spark}
2019-10-12 23:37:35,672 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@de18f63{/jobs/job,null,AVAILABLE,@Spark}
2019-10-12 23:37:35,674 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@4e904fd5{/jobs/job/json,null,AVAILABLE,@Spark}
2019-10-12 23:37:35,675 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@ea9e141{/stages,null,AVAILABLE,@Spark}
2019-10-12 23:37:35,676 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@5c748168{/stages/json,null,AVAILABLE,@Spark}
2019-10-12 23:37:35,677 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@31c2affc{/stages/stage,null,AVAILABLE,@Spark}
2019-10-12 23:37:35,679 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@7c4fc2bf{/stages/stage/json,null,AVAILABLE,@Spark}
2019-10-12 23:37:35,682 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@236134a1{/stages/pool,null,AVAILABLE,@Spark}
2019-10-12 23:37:35,683 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@68dcfd52{/stages/pool/json,null,AVAILABLE,@Spark}
2019-10-12 23:37:35,685 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@66e8997c{/storage,null,AVAILABLE,@Spark}
2019-10-12 23:37:35,686 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@655523dd{/storage/json,null,AVAILABLE,@Spark}
2019-10-12 23:37:35,689 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@c6e0f32{/storage/rdd,null,AVAILABLE,@Spark}
2019-10-12 23:37:35,690 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@63fdffcd{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-10-12 23:37:35,691 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@50a3d0f6{/environment,null,AVAILABLE,@Spark}
2019-10-12 23:37:35,693 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@24e08d59{/environment/json,null,AVAILABLE,@Spark}
2019-10-12 23:37:35,694 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@e04ccf8{/executors,null,AVAILABLE,@Spark}
2019-10-12 23:37:35,696 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@6e0cff20{/executors/json,null,AVAILABLE,@Spark}
2019-10-12 23:37:35,698 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@77c7ed8e{/executors/threadDump,null,AVAILABLE,@Spark}
2019-10-12 23:37:35,699 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@640dc4c6{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-10-12 23:37:35,706 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@44de94c3{/static,null,AVAILABLE,@Spark}
2019-10-12 23:37:35,708 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@11a00961{/,null,AVAILABLE,@Spark}
2019-10-12 23:37:35,709 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@53ac845a{/api,null,AVAILABLE,@Spark}
2019-10-12 23:37:35,711 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@26d820eb{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-10-12 23:37:35,714 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@49293b43{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-10-12 23:37:35,715 [main] INFO [org.apache.spark.ui.SparkUI] - Bound SparkUI to 0.0.0.0, and started at http://192.168.200.1:4040
2019-10-12 23:37:35,793 [main] INFO [org.apache.spark.executor.Executor] - Starting executor ID driver on host localhost
2019-10-12 23:37:35,814 [main] INFO [org.apache.spark.util.Utils] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 65450.
2019-10-12 23:37:35,814 [main] INFO [org.apache.spark.network.netty.NettyBlockTransferService] - Server created on 192.168.200.1:65450
2019-10-12 23:37:35,816 [main] INFO [org.apache.spark.storage.BlockManager] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-10-12 23:37:35,817 [main] INFO [org.apache.spark.storage.BlockManagerMaster] - Registering BlockManager BlockManagerId(driver, 192.168.200.1, 65450, None)
2019-10-12 23:37:35,821 [dispatcher-event-loop-2] INFO [org.apache.spark.storage.BlockManagerMasterEndpoint] - Registering block manager 192.168.200.1:65450 with 1989.6 MB RAM, BlockManagerId(driver, 192.168.200.1, 65450, None)
2019-10-12 23:37:35,822 [main] INFO [org.apache.spark.storage.BlockManagerMaster] - Registered BlockManager BlockManagerId(driver, 192.168.200.1, 65450, None)
2019-10-12 23:37:35,823 [main] INFO [org.apache.spark.storage.BlockManager] - Initialized BlockManager: BlockManagerId(driver, 192.168.200.1, 65450, None)
2019-10-12 23:37:35,989 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1981d861{/metrics/json,null,AVAILABLE,@Spark}
2019-10-12 23:37:36,062 [main] INFO [org.apache.spark.sql.internal.SharedState] - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/E:/StudyingCourse/IntellijModule/tech-climbing-common/spark-warehouse/').
2019-10-12 23:37:36,062 [main] INFO [org.apache.spark.sql.internal.SharedState] - Warehouse path is 'file:/E:/StudyingCourse/IntellijModule/tech-climbing-common/spark-warehouse/'.
2019-10-12 23:37:36,069 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@7a8b9166{/SQL,null,AVAILABLE,@Spark}
2019-10-12 23:37:36,073 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@56cfe111{/SQL/json,null,AVAILABLE,@Spark}
2019-10-12 23:37:36,075 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@7c847072{/SQL/execution,null,AVAILABLE,@Spark}
2019-10-12 23:37:36,076 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@7d3c09ec{/SQL/execution/json,null,AVAILABLE,@Spark}
2019-10-12 23:37:36,078 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@30ca0779{/static/sql,null,AVAILABLE,@Spark}
2019-10-12 23:37:36,699 [main] INFO [org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef] - Registered StateStoreCoordinator endpoint
2019-10-12 23:37:37,047 [main] INFO [org.apache.spark.SparkContext] - Starting job: foreach at RDDTwo.scala:62
2019-10-12 23:37:37,249 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Registering RDD 0 (parallelize at RDDTwo.scala:59)
2019-10-12 23:37:37,251 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Got job 0 (foreach at RDDTwo.scala:62) with 1 output partitions
2019-10-12 23:37:37,251 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 1 (foreach at RDDTwo.scala:62)
2019-10-12 23:37:37,251 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List(ShuffleMapStage 0)
2019-10-12 23:37:37,252 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List(ShuffleMapStage 0)
2019-10-12 23:37:37,259 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting ShuffleMapStage 0 (ParallelCollectionRDD[0] at parallelize at RDDTwo.scala:59), which has no missing parents
2019-10-12 23:37:37,366 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_0 stored as values in memory (estimated size 2.5 KB, free 1989.6 MB)
2019-10-12 23:37:37,418 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 1659.0 B, free 1989.6 MB)
2019-10-12 23:37:37,420 [dispatcher-event-loop-0] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_0_piece0 in memory on 192.168.200.1:65450 (size: 1659.0 B, free: 1989.6 MB)
2019-10-12 23:37:37,422 [dag-scheduler-event-loop] INFO [org.apache.spark.SparkContext] - Created broadcast 0 from broadcast at DAGScheduler.scala:1006
2019-10-12 23:37:37,488 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting 1 missing tasks from ShuffleMapStage 0 (ParallelCollectionRDD[0] at parallelize at RDDTwo.scala:59) (first 15 tasks are for partitions Vector(0))
2019-10-12 23:37:37,488 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 0.0 with 1 tasks
2019-10-12 23:37:37,532 [dispatcher-event-loop-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4947 bytes)
2019-10-12 23:37:37,539 [Executor task launch worker for task 0] INFO [org.apache.spark.executor.Executor] - Running task 0.0 in stage 0.0 (TID 0)
2019-10-12 23:37:37,676 [Executor task launch worker for task 0] INFO [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 0.0 (TID 0). 1068 bytes result sent to driver
2019-10-12 23:37:37,692 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 0.0 (TID 0) in 177 ms on localhost (executor driver) (1/1)
2019-10-12 23:37:37,694 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-10-12 23:37:37,699 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - ShuffleMapStage 0 (parallelize at RDDTwo.scala:59) finished in 0.196 s
2019-10-12 23:37:37,700 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - looking for newly runnable stages
2019-10-12 23:37:37,700 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - running: Set()
2019-10-12 23:37:37,701 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - waiting: Set(ResultStage 1)
2019-10-12 23:37:37,702 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - failed: Set()
2019-10-12 23:37:37,706 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 1 (ShuffledRDD[1] at reduceByKey at RDDTwo.scala:61), which has no missing parents
2019-10-12 23:37:37,718 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1 stored as values in memory (estimated size 3.1 KB, free 1989.6 MB)
2019-10-12 23:37:37,720 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1_piece0 stored as bytes in memory (estimated size 1958.0 B, free 1989.6 MB)
2019-10-12 23:37:37,721 [dispatcher-event-loop-0] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_1_piece0 in memory on 192.168.200.1:65450 (size: 1958.0 B, free: 1989.6 MB)
2019-10-12 23:37:37,722 [dag-scheduler-event-loop] INFO [org.apache.spark.SparkContext] - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
2019-10-12 23:37:37,724 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting 1 missing tasks from ResultStage 1 (ShuffledRDD[1] at reduceByKey at RDDTwo.scala:61) (first 15 tasks are for partitions Vector(0))
2019-10-12 23:37:37,724 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 1.0 with 1 tasks
2019-10-12 23:37:37,728 [dispatcher-event-loop-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, ANY, 4621 bytes)
2019-10-12 23:37:37,728 [Executor task launch worker for task 1] INFO [org.apache.spark.executor.Executor] - Running task 0.0 in stage 1.0 (TID 1)
2019-10-12 23:37:37,742 [Executor task launch worker for task 1] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 1 non-empty blocks out of 1 blocks
2019-10-12 23:37:37,743 [Executor task launch worker for task 1] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 4 ms
2019-10-12 23:37:37,817 [Executor task launch worker for task 1] INFO [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 1.0 (TID 1). 1095 bytes result sent to driver
2019-10-12 23:37:37,818 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 1.0 (TID 1) in 92 ms on localhost (executor driver) (1/1)
2019-10-12 23:37:37,818 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2019-10-12 23:37:37,819 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - ResultStage 1 (foreach at RDDTwo.scala:62) finished in 0.093 s
2019-10-12 23:37:37,823 [main] INFO [org.apache.spark.scheduler.DAGScheduler] - Job 0 finished: foreach at RDDTwo.scala:62, took 0.775503 s
2019-10-12 23:37:37,827 [Thread-1] INFO [org.apache.spark.SparkContext] - Invoking stop() from shutdown hook
2019-10-12 23:37:37,835 [Thread-1] INFO [org.spark_project.jetty.server.AbstractConnector] - Stopped Spark@51351f28{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-10-12 23:37:37,837 [Thread-1] INFO [org.apache.spark.ui.SparkUI] - Stopped Spark web UI at http://192.168.200.1:4040
2019-10-12 23:37:37,857 [dispatcher-event-loop-3] INFO [org.apache.spark.MapOutputTrackerMasterEndpoint] - MapOutputTrackerMasterEndpoint stopped!
2019-10-12 23:37:37,872 [Thread-1] INFO [org.apache.spark.storage.memory.MemoryStore] - MemoryStore cleared
2019-10-12 23:37:37,873 [Thread-1] INFO [org.apache.spark.storage.BlockManager] - BlockManager stopped
2019-10-12 23:37:37,877 [Thread-1] INFO [org.apache.spark.storage.BlockManagerMaster] - BlockManagerMaster stopped
2019-10-12 23:37:37,878 [dispatcher-event-loop-1] INFO [org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint] - OutputCommitCoordinator stopped!
2019-10-12 23:37:37,881 [Thread-1] INFO [org.apache.spark.SparkContext] - Successfully stopped SparkContext
2019-10-12 23:37:37,882 [Thread-1] INFO [org.apache.spark.util.ShutdownHookManager] - Shutdown hook called
2019-10-12 23:37:37,882 [Thread-1] INFO [org.apache.spark.util.ShutdownHookManager] - Deleting directory C:\Users\FrankCooper\AppData\Local\Temp\spark-ee966d5a-4271-42d9-9908-608f37205b53
2019-10-12 23:39:32,901 [main] INFO [org.apache.spark.SparkContext] - Running Spark version 2.2.0
2019-10-12 23:39:33,208 [main] WARN [org.apache.hadoop.util.NativeCodeLoader] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-10-12 23:39:33,426 [main] INFO [org.apache.spark.SparkContext] - Submitted application: RDDTwo
2019-10-12 23:39:33,443 [main] INFO [org.apache.spark.SecurityManager] - Changing view acls to: FrankCooper
2019-10-12 23:39:33,443 [main] INFO [org.apache.spark.SecurityManager] - Changing modify acls to: FrankCooper
2019-10-12 23:39:33,444 [main] INFO [org.apache.spark.SecurityManager] - Changing view acls groups to: 
2019-10-12 23:39:33,444 [main] INFO [org.apache.spark.SecurityManager] - Changing modify acls groups to: 
2019-10-12 23:39:33,444 [main] INFO [org.apache.spark.SecurityManager] - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(FrankCooper); groups with view permissions: Set(); users  with modify permissions: Set(FrankCooper); groups with modify permissions: Set()
2019-10-12 23:39:33,947 [main] INFO [org.apache.spark.util.Utils] - Successfully started service 'sparkDriver' on port 65497.
2019-10-12 23:39:33,965 [main] INFO [org.apache.spark.SparkEnv] - Registering MapOutputTracker
2019-10-12 23:39:33,983 [main] INFO [org.apache.spark.SparkEnv] - Registering BlockManagerMaster
2019-10-12 23:39:33,986 [main] INFO [org.apache.spark.storage.BlockManagerMasterEndpoint] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-10-12 23:39:33,987 [main] INFO [org.apache.spark.storage.BlockManagerMasterEndpoint] - BlockManagerMasterEndpoint up
2019-10-12 23:39:33,996 [main] INFO [org.apache.spark.storage.DiskBlockManager] - Created local directory at C:\Users\FrankCooper\AppData\Local\Temp\blockmgr-a4214f76-9195-46df-aa78-3c0113385ea2
2019-10-12 23:39:34,043 [main] INFO [org.apache.spark.storage.memory.MemoryStore] - MemoryStore started with capacity 1989.6 MB
2019-10-12 23:39:34,094 [main] INFO [org.apache.spark.SparkEnv] - Registering OutputCommitCoordinator
2019-10-12 23:39:34,164 [main] INFO [org.spark_project.jetty.util.log] - Logging initialized @2940ms
2019-10-12 23:39:34,224 [main] INFO [org.spark_project.jetty.server.Server] - jetty-9.3.z-SNAPSHOT
2019-10-12 23:39:34,237 [main] INFO [org.spark_project.jetty.server.Server] - Started @3012ms
2019-10-12 23:39:34,257 [main] INFO [org.spark_project.jetty.server.AbstractConnector] - Started ServerConnector@34cf294c{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-10-12 23:39:34,258 [main] INFO [org.apache.spark.util.Utils] - Successfully started service 'SparkUI' on port 4040.
2019-10-12 23:39:34,282 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@253b380a{/jobs,null,AVAILABLE,@Spark}
2019-10-12 23:39:34,283 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@34a0ef00{/jobs/json,null,AVAILABLE,@Spark}
2019-10-12 23:39:34,284 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@4f8b4bd0{/jobs/job,null,AVAILABLE,@Spark}
2019-10-12 23:39:34,286 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@597f48df{/jobs/job/json,null,AVAILABLE,@Spark}
2019-10-12 23:39:34,287 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1d96d872{/stages,null,AVAILABLE,@Spark}
2019-10-12 23:39:34,288 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@234a8f27{/stages/json,null,AVAILABLE,@Spark}
2019-10-12 23:39:34,289 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@58d63b16{/stages/stage,null,AVAILABLE,@Spark}
2019-10-12 23:39:34,291 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@46866946{/stages/stage/json,null,AVAILABLE,@Spark}
2019-10-12 23:39:34,292 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@42b21d99{/stages/pool,null,AVAILABLE,@Spark}
2019-10-12 23:39:34,294 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@37b72ea{/stages/pool/json,null,AVAILABLE,@Spark}
2019-10-12 23:39:34,295 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@47547132{/storage,null,AVAILABLE,@Spark}
2019-10-12 23:39:34,297 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@5e1218b4{/storage/json,null,AVAILABLE,@Spark}
2019-10-12 23:39:34,298 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1f966492{/storage/rdd,null,AVAILABLE,@Spark}
2019-10-12 23:39:34,299 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@4a68135e{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-10-12 23:39:34,300 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@6a0ac48e{/environment,null,AVAILABLE,@Spark}
2019-10-12 23:39:34,302 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@35636217{/environment/json,null,AVAILABLE,@Spark}
2019-10-12 23:39:34,303 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@4647881c{/executors,null,AVAILABLE,@Spark}
2019-10-12 23:39:34,304 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@10667848{/executors/json,null,AVAILABLE,@Spark}
2019-10-12 23:39:34,305 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@2c6ee758{/executors/threadDump,null,AVAILABLE,@Spark}
2019-10-12 23:39:34,307 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@203c20cf{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-10-12 23:39:34,313 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@2116b68b{/static,null,AVAILABLE,@Spark}
2019-10-12 23:39:34,314 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@6fd5717c{/,null,AVAILABLE,@Spark}
2019-10-12 23:39:34,316 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@3fdecce{/api,null,AVAILABLE,@Spark}
2019-10-12 23:39:34,316 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@5cbd159f{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-10-12 23:39:34,318 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@67b7c170{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-10-12 23:39:34,319 [main] INFO [org.apache.spark.ui.SparkUI] - Bound SparkUI to 0.0.0.0, and started at http://192.168.200.1:4040
2019-10-12 23:39:34,387 [main] INFO [org.apache.spark.executor.Executor] - Starting executor ID driver on host localhost
2019-10-12 23:39:34,406 [main] INFO [org.apache.spark.util.Utils] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 65506.
2019-10-12 23:39:34,406 [main] INFO [org.apache.spark.network.netty.NettyBlockTransferService] - Server created on 192.168.200.1:65506
2019-10-12 23:39:34,408 [main] INFO [org.apache.spark.storage.BlockManager] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-10-12 23:39:34,409 [main] INFO [org.apache.spark.storage.BlockManagerMaster] - Registering BlockManager BlockManagerId(driver, 192.168.200.1, 65506, None)
2019-10-12 23:39:34,412 [dispatcher-event-loop-2] INFO [org.apache.spark.storage.BlockManagerMasterEndpoint] - Registering block manager 192.168.200.1:65506 with 1989.6 MB RAM, BlockManagerId(driver, 192.168.200.1, 65506, None)
2019-10-12 23:39:34,414 [main] INFO [org.apache.spark.storage.BlockManagerMaster] - Registered BlockManager BlockManagerId(driver, 192.168.200.1, 65506, None)
2019-10-12 23:39:34,414 [main] INFO [org.apache.spark.storage.BlockManager] - Initialized BlockManager: BlockManagerId(driver, 192.168.200.1, 65506, None)
2019-10-12 23:39:34,547 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@74174a23{/metrics/json,null,AVAILABLE,@Spark}
2019-10-12 23:39:34,611 [main] INFO [org.apache.spark.sql.internal.SharedState] - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/E:/StudyingCourse/IntellijModule/tech-climbing-common/spark-warehouse/').
2019-10-12 23:39:34,611 [main] INFO [org.apache.spark.sql.internal.SharedState] - Warehouse path is 'file:/E:/StudyingCourse/IntellijModule/tech-climbing-common/spark-warehouse/'.
2019-10-12 23:39:34,616 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@57f9b467{/SQL,null,AVAILABLE,@Spark}
2019-10-12 23:39:34,617 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@3078cac{/SQL/json,null,AVAILABLE,@Spark}
2019-10-12 23:39:34,620 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@11900483{/SQL/execution,null,AVAILABLE,@Spark}
2019-10-12 23:39:34,621 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@4cc36c19{/SQL/execution/json,null,AVAILABLE,@Spark}
2019-10-12 23:39:34,622 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@4b74b35{/static/sql,null,AVAILABLE,@Spark}
2019-10-12 23:39:35,214 [main] INFO [org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef] - Registered StateStoreCoordinator endpoint
2019-10-12 23:39:35,610 [main] INFO [org.apache.spark.SparkContext] - Starting job: foreach at RDDTwo.scala:71
2019-10-12 23:39:35,630 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Registering RDD 0 (parallelize at RDDTwo.scala:69)
2019-10-12 23:39:35,632 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Got job 0 (foreach at RDDTwo.scala:71) with 1 output partitions
2019-10-12 23:39:35,632 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 1 (foreach at RDDTwo.scala:71)
2019-10-12 23:39:35,633 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List(ShuffleMapStage 0)
2019-10-12 23:39:35,633 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List(ShuffleMapStage 0)
2019-10-12 23:39:35,638 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting ShuffleMapStage 0 (ParallelCollectionRDD[0] at parallelize at RDDTwo.scala:69), which has no missing parents
2019-10-12 23:39:35,731 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_0 stored as values in memory (estimated size 3.3 KB, free 1989.6 MB)
2019-10-12 23:39:35,779 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 1930.0 B, free 1989.6 MB)
2019-10-12 23:39:35,782 [dispatcher-event-loop-1] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_0_piece0 in memory on 192.168.200.1:65506 (size: 1930.0 B, free: 1989.6 MB)
2019-10-12 23:39:35,784 [dag-scheduler-event-loop] INFO [org.apache.spark.SparkContext] - Created broadcast 0 from broadcast at DAGScheduler.scala:1006
2019-10-12 23:39:35,798 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting 1 missing tasks from ShuffleMapStage 0 (ParallelCollectionRDD[0] at parallelize at RDDTwo.scala:69) (first 15 tasks are for partitions Vector(0))
2019-10-12 23:39:35,798 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 0.0 with 1 tasks
2019-10-12 23:39:35,833 [dispatcher-event-loop-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4958 bytes)
2019-10-12 23:39:35,840 [Executor task launch worker for task 0] INFO [org.apache.spark.executor.Executor] - Running task 0.0 in stage 0.0 (TID 0)
2019-10-12 23:39:36,200 [Executor task launch worker for task 0] INFO [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 0.0 (TID 0). 896 bytes result sent to driver
2019-10-12 23:39:36,209 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 0.0 (TID 0) in 388 ms on localhost (executor driver) (1/1)
2019-10-12 23:39:36,211 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-10-12 23:39:36,217 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - ShuffleMapStage 0 (parallelize at RDDTwo.scala:69) finished in 0.406 s
2019-10-12 23:39:36,217 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - looking for newly runnable stages
2019-10-12 23:39:36,218 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - running: Set()
2019-10-12 23:39:36,218 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - waiting: Set(ResultStage 1)
2019-10-12 23:39:36,219 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - failed: Set()
2019-10-12 23:39:36,224 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 1 (ShuffledRDD[1] at groupByKey at RDDTwo.scala:70), which has no missing parents
2019-10-12 23:39:36,237 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1 stored as values in memory (estimated size 4.2 KB, free 1989.6 MB)
2019-10-12 23:39:36,243 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.3 KB, free 1989.6 MB)
2019-10-12 23:39:36,246 [dispatcher-event-loop-1] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_1_piece0 in memory on 192.168.200.1:65506 (size: 2.3 KB, free: 1989.6 MB)
2019-10-12 23:39:36,247 [dag-scheduler-event-loop] INFO [org.apache.spark.SparkContext] - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
2019-10-12 23:39:36,250 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting 1 missing tasks from ResultStage 1 (ShuffledRDD[1] at groupByKey at RDDTwo.scala:70) (first 15 tasks are for partitions Vector(0))
2019-10-12 23:39:36,250 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 1.0 with 1 tasks
2019-10-12 23:39:36,255 [dispatcher-event-loop-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, ANY, 4621 bytes)
2019-10-12 23:39:36,255 [Executor task launch worker for task 1] INFO [org.apache.spark.executor.Executor] - Running task 0.0 in stage 1.0 (TID 1)
2019-10-12 23:39:36,269 [Executor task launch worker for task 1] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 1 non-empty blocks out of 1 blocks
2019-10-12 23:39:36,271 [Executor task launch worker for task 1] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 5 ms
2019-10-12 23:40:08,145 [Executor task launch worker for task 1] INFO [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 1.0 (TID 1). 1138 bytes result sent to driver
2019-10-12 23:40:08,148 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 1.0 (TID 1) in 31895 ms on localhost (executor driver) (1/1)
2019-10-12 23:40:08,148 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - ResultStage 1 (foreach at RDDTwo.scala:71) finished in 31.894 s
2019-10-12 23:40:08,149 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2019-10-12 23:40:08,153 [main] INFO [org.apache.spark.scheduler.DAGScheduler] - Job 0 finished: foreach at RDDTwo.scala:71, took 32.543029 s
2019-10-12 23:40:08,161 [Thread-1] INFO [org.apache.spark.SparkContext] - Invoking stop() from shutdown hook
2019-10-12 23:40:08,166 [Thread-1] INFO [org.spark_project.jetty.server.AbstractConnector] - Stopped Spark@34cf294c{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-10-12 23:40:08,170 [Thread-1] INFO [org.apache.spark.ui.SparkUI] - Stopped Spark web UI at http://192.168.200.1:4040
2019-10-12 23:40:08,190 [dispatcher-event-loop-3] INFO [org.apache.spark.MapOutputTrackerMasterEndpoint] - MapOutputTrackerMasterEndpoint stopped!
2019-10-12 23:40:08,208 [Thread-1] INFO [org.apache.spark.storage.memory.MemoryStore] - MemoryStore cleared
2019-10-12 23:40:08,209 [Thread-1] INFO [org.apache.spark.storage.BlockManager] - BlockManager stopped
2019-10-12 23:40:08,209 [Thread-1] INFO [org.apache.spark.storage.BlockManagerMaster] - BlockManagerMaster stopped
2019-10-12 23:40:08,210 [dispatcher-event-loop-0] INFO [org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint] - OutputCommitCoordinator stopped!
2019-10-12 23:40:08,213 [Thread-1] INFO [org.apache.spark.SparkContext] - Successfully stopped SparkContext
2019-10-12 23:40:08,213 [Thread-1] INFO [org.apache.spark.util.ShutdownHookManager] - Shutdown hook called
2019-10-12 23:40:08,213 [Thread-1] INFO [org.apache.spark.util.ShutdownHookManager] - Deleting directory C:\Users\FrankCooper\AppData\Local\Temp\spark-3e4284d9-52a7-4eff-ba0d-5ac58a5eac36
2019-10-12 23:40:12,970 [main] INFO [org.apache.spark.SparkContext] - Running Spark version 2.2.0
2019-10-12 23:40:13,292 [main] WARN [org.apache.hadoop.util.NativeCodeLoader] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-10-12 23:40:13,449 [main] INFO [org.apache.spark.SparkContext] - Submitted application: RDDTwo
2019-10-12 23:40:13,464 [main] INFO [org.apache.spark.SecurityManager] - Changing view acls to: FrankCooper
2019-10-12 23:40:13,464 [main] INFO [org.apache.spark.SecurityManager] - Changing modify acls to: FrankCooper
2019-10-12 23:40:13,465 [main] INFO [org.apache.spark.SecurityManager] - Changing view acls groups to: 
2019-10-12 23:40:13,465 [main] INFO [org.apache.spark.SecurityManager] - Changing modify acls groups to: 
2019-10-12 23:40:13,466 [main] INFO [org.apache.spark.SecurityManager] - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(FrankCooper); groups with view permissions: Set(); users  with modify permissions: Set(FrankCooper); groups with modify permissions: Set()
2019-10-12 23:40:13,946 [main] INFO [org.apache.spark.util.Utils] - Successfully started service 'sparkDriver' on port 49154.
2019-10-12 23:40:13,963 [main] INFO [org.apache.spark.SparkEnv] - Registering MapOutputTracker
2019-10-12 23:40:13,979 [main] INFO [org.apache.spark.SparkEnv] - Registering BlockManagerMaster
2019-10-12 23:40:13,982 [main] INFO [org.apache.spark.storage.BlockManagerMasterEndpoint] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-10-12 23:40:13,983 [main] INFO [org.apache.spark.storage.BlockManagerMasterEndpoint] - BlockManagerMasterEndpoint up
2019-10-12 23:40:13,992 [main] INFO [org.apache.spark.storage.DiskBlockManager] - Created local directory at C:\Users\FrankCooper\AppData\Local\Temp\blockmgr-5ed8832a-15ae-4440-bbaf-a8204f148673
2019-10-12 23:40:14,037 [main] INFO [org.apache.spark.storage.memory.MemoryStore] - MemoryStore started with capacity 1989.6 MB
2019-10-12 23:40:14,080 [main] INFO [org.apache.spark.SparkEnv] - Registering OutputCommitCoordinator
2019-10-12 23:40:14,151 [main] INFO [org.spark_project.jetty.util.log] - Logging initialized @2670ms
2019-10-12 23:40:14,212 [main] INFO [org.spark_project.jetty.server.Server] - jetty-9.3.z-SNAPSHOT
2019-10-12 23:40:14,225 [main] INFO [org.spark_project.jetty.server.Server] - Started @2744ms
2019-10-12 23:40:14,246 [main] INFO [org.spark_project.jetty.server.AbstractConnector] - Started ServerConnector@51351f28{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-10-12 23:40:14,246 [main] INFO [org.apache.spark.util.Utils] - Successfully started service 'SparkUI' on port 4040.
2019-10-12 23:40:14,272 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@42f9c19a{/jobs,null,AVAILABLE,@Spark}
2019-10-12 23:40:14,274 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1a3e5f23{/jobs/json,null,AVAILABLE,@Spark}
2019-10-12 23:40:14,275 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@de18f63{/jobs/job,null,AVAILABLE,@Spark}
2019-10-12 23:40:14,276 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@4e904fd5{/jobs/job/json,null,AVAILABLE,@Spark}
2019-10-12 23:40:14,277 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@ea9e141{/stages,null,AVAILABLE,@Spark}
2019-10-12 23:40:14,279 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@5c748168{/stages/json,null,AVAILABLE,@Spark}
2019-10-12 23:40:14,280 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@31c2affc{/stages/stage,null,AVAILABLE,@Spark}
2019-10-12 23:40:14,282 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@7c4fc2bf{/stages/stage/json,null,AVAILABLE,@Spark}
2019-10-12 23:40:14,283 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@236134a1{/stages/pool,null,AVAILABLE,@Spark}
2019-10-12 23:40:14,284 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@68dcfd52{/stages/pool/json,null,AVAILABLE,@Spark}
2019-10-12 23:40:14,286 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@66e8997c{/storage,null,AVAILABLE,@Spark}
2019-10-12 23:40:14,287 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@655523dd{/storage/json,null,AVAILABLE,@Spark}
2019-10-12 23:40:14,288 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@c6e0f32{/storage/rdd,null,AVAILABLE,@Spark}
2019-10-12 23:40:14,289 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@63fdffcd{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-10-12 23:40:14,290 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@50a3d0f6{/environment,null,AVAILABLE,@Spark}
2019-10-12 23:40:14,292 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@24e08d59{/environment/json,null,AVAILABLE,@Spark}
2019-10-12 23:40:14,293 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@e04ccf8{/executors,null,AVAILABLE,@Spark}
2019-10-12 23:40:14,294 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@6e0cff20{/executors/json,null,AVAILABLE,@Spark}
2019-10-12 23:40:14,296 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@77c7ed8e{/executors/threadDump,null,AVAILABLE,@Spark}
2019-10-12 23:40:14,297 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@640dc4c6{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-10-12 23:40:14,303 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@44de94c3{/static,null,AVAILABLE,@Spark}
2019-10-12 23:40:14,305 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@11a00961{/,null,AVAILABLE,@Spark}
2019-10-12 23:40:14,307 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@53ac845a{/api,null,AVAILABLE,@Spark}
2019-10-12 23:40:14,308 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@26d820eb{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-10-12 23:40:14,309 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@49293b43{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-10-12 23:40:14,311 [main] INFO [org.apache.spark.ui.SparkUI] - Bound SparkUI to 0.0.0.0, and started at http://192.168.200.1:4040
2019-10-12 23:40:14,380 [main] INFO [org.apache.spark.executor.Executor] - Starting executor ID driver on host localhost
2019-10-12 23:40:14,398 [main] INFO [org.apache.spark.util.Utils] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 49163.
2019-10-12 23:40:14,399 [main] INFO [org.apache.spark.network.netty.NettyBlockTransferService] - Server created on 192.168.200.1:49163
2019-10-12 23:40:14,400 [main] INFO [org.apache.spark.storage.BlockManager] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-10-12 23:40:14,401 [main] INFO [org.apache.spark.storage.BlockManagerMaster] - Registering BlockManager BlockManagerId(driver, 192.168.200.1, 49163, None)
2019-10-12 23:40:14,404 [dispatcher-event-loop-1] INFO [org.apache.spark.storage.BlockManagerMasterEndpoint] - Registering block manager 192.168.200.1:49163 with 1989.6 MB RAM, BlockManagerId(driver, 192.168.200.1, 49163, None)
2019-10-12 23:40:14,407 [main] INFO [org.apache.spark.storage.BlockManagerMaster] - Registered BlockManager BlockManagerId(driver, 192.168.200.1, 49163, None)
2019-10-12 23:40:14,407 [main] INFO [org.apache.spark.storage.BlockManager] - Initialized BlockManager: BlockManagerId(driver, 192.168.200.1, 49163, None)
2019-10-12 23:40:14,545 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1981d861{/metrics/json,null,AVAILABLE,@Spark}
2019-10-12 23:40:14,626 [main] INFO [org.apache.spark.sql.internal.SharedState] - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/E:/StudyingCourse/IntellijModule/tech-climbing-common/spark-warehouse/').
2019-10-12 23:40:14,626 [main] INFO [org.apache.spark.sql.internal.SharedState] - Warehouse path is 'file:/E:/StudyingCourse/IntellijModule/tech-climbing-common/spark-warehouse/'.
2019-10-12 23:40:14,632 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@3078cac{/SQL,null,AVAILABLE,@Spark}
2019-10-12 23:40:14,633 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@43d9f1a2{/SQL/json,null,AVAILABLE,@Spark}
2019-10-12 23:40:14,633 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@4cc36c19{/SQL/execution,null,AVAILABLE,@Spark}
2019-10-12 23:40:14,635 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1d805aa1{/SQL/execution/json,null,AVAILABLE,@Spark}
2019-10-12 23:40:14,638 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@72e295cc{/static/sql,null,AVAILABLE,@Spark}
2019-10-12 23:40:15,249 [main] INFO [org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef] - Registered StateStoreCoordinator endpoint
2019-10-12 23:40:15,495 [main] INFO [org.apache.spark.SparkContext] - Starting job: foreach at RDDTwo.scala:71
2019-10-12 23:40:15,514 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Registering RDD 0 (parallelize at RDDTwo.scala:69)
2019-10-12 23:40:15,515 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Got job 0 (foreach at RDDTwo.scala:71) with 1 output partitions
2019-10-12 23:40:15,516 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 1 (foreach at RDDTwo.scala:71)
2019-10-12 23:40:15,516 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List(ShuffleMapStage 0)
2019-10-12 23:40:15,517 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List(ShuffleMapStage 0)
2019-10-12 23:40:15,521 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting ShuffleMapStage 0 (ParallelCollectionRDD[0] at parallelize at RDDTwo.scala:69), which has no missing parents
2019-10-12 23:40:15,603 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_0 stored as values in memory (estimated size 3.3 KB, free 1989.6 MB)
2019-10-12 23:40:15,647 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 1930.0 B, free 1989.6 MB)
2019-10-12 23:40:15,649 [dispatcher-event-loop-2] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_0_piece0 in memory on 192.168.200.1:49163 (size: 1930.0 B, free: 1989.6 MB)
2019-10-12 23:40:15,732 [dag-scheduler-event-loop] INFO [org.apache.spark.SparkContext] - Created broadcast 0 from broadcast at DAGScheduler.scala:1006
2019-10-12 23:40:15,762 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting 1 missing tasks from ShuffleMapStage 0 (ParallelCollectionRDD[0] at parallelize at RDDTwo.scala:69) (first 15 tasks are for partitions Vector(0))
2019-10-12 23:40:15,763 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 0.0 with 1 tasks
2019-10-12 23:40:15,803 [dispatcher-event-loop-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4958 bytes)
2019-10-12 23:40:15,809 [Executor task launch worker for task 0] INFO [org.apache.spark.executor.Executor] - Running task 0.0 in stage 0.0 (TID 0)
2019-10-12 23:40:16,005 [Executor task launch worker for task 0] INFO [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 0.0 (TID 0). 896 bytes result sent to driver
2019-10-12 23:40:16,015 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 0.0 (TID 0) in 226 ms on localhost (executor driver) (1/1)
2019-10-12 23:40:16,019 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-10-12 23:40:16,023 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - ShuffleMapStage 0 (parallelize at RDDTwo.scala:69) finished in 0.245 s
2019-10-12 23:40:16,024 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - looking for newly runnable stages
2019-10-12 23:40:16,024 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - running: Set()
2019-10-12 23:40:16,025 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - waiting: Set(ResultStage 1)
2019-10-12 23:40:16,025 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - failed: Set()
2019-10-12 23:40:16,029 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 1 (ShuffledRDD[1] at groupByKey at RDDTwo.scala:70), which has no missing parents
2019-10-12 23:40:16,042 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1 stored as values in memory (estimated size 4.2 KB, free 1989.6 MB)
2019-10-12 23:40:16,046 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.3 KB, free 1989.6 MB)
2019-10-12 23:40:16,048 [dispatcher-event-loop-2] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_1_piece0 in memory on 192.168.200.1:49163 (size: 2.3 KB, free: 1989.6 MB)
2019-10-12 23:40:16,048 [dag-scheduler-event-loop] INFO [org.apache.spark.SparkContext] - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
2019-10-12 23:40:16,050 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting 1 missing tasks from ResultStage 1 (ShuffledRDD[1] at groupByKey at RDDTwo.scala:70) (first 15 tasks are for partitions Vector(0))
2019-10-12 23:40:16,050 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 1.0 with 1 tasks
2019-10-12 23:40:16,055 [dispatcher-event-loop-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, ANY, 4621 bytes)
2019-10-12 23:40:16,055 [Executor task launch worker for task 1] INFO [org.apache.spark.executor.Executor] - Running task 0.0 in stage 1.0 (TID 1)
2019-10-12 23:40:16,067 [Executor task launch worker for task 1] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 1 non-empty blocks out of 1 blocks
2019-10-12 23:40:16,068 [Executor task launch worker for task 1] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 4 ms
2019-10-12 23:40:24,751 [Executor task launch worker for task 1] INFO [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 1.0 (TID 1). 1138 bytes result sent to driver
2019-10-12 23:40:24,753 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 1.0 (TID 1) in 8699 ms on localhost (executor driver) (1/1)
2019-10-12 23:40:24,753 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2019-10-12 23:40:24,754 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - ResultStage 1 (foreach at RDDTwo.scala:71) finished in 8.700 s
2019-10-12 23:40:24,759 [main] INFO [org.apache.spark.scheduler.DAGScheduler] - Job 0 finished: foreach at RDDTwo.scala:71, took 9.264142 s
2019-10-12 23:40:24,762 [Thread-1] INFO [org.apache.spark.SparkContext] - Invoking stop() from shutdown hook
2019-10-12 23:40:24,768 [Thread-1] INFO [org.spark_project.jetty.server.AbstractConnector] - Stopped Spark@51351f28{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-10-12 23:40:24,770 [Thread-1] INFO [org.apache.spark.ui.SparkUI] - Stopped Spark web UI at http://192.168.200.1:4040
2019-10-12 23:40:24,807 [dispatcher-event-loop-3] INFO [org.apache.spark.MapOutputTrackerMasterEndpoint] - MapOutputTrackerMasterEndpoint stopped!
2019-10-12 23:40:24,826 [Thread-1] INFO [org.apache.spark.storage.memory.MemoryStore] - MemoryStore cleared
2019-10-12 23:40:24,827 [Thread-1] INFO [org.apache.spark.storage.BlockManager] - BlockManager stopped
2019-10-12 23:40:24,831 [Thread-1] INFO [org.apache.spark.storage.BlockManagerMaster] - BlockManagerMaster stopped
2019-10-12 23:40:24,833 [dispatcher-event-loop-0] INFO [org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint] - OutputCommitCoordinator stopped!
2019-10-12 23:40:24,836 [Thread-1] INFO [org.apache.spark.SparkContext] - Successfully stopped SparkContext
2019-10-12 23:40:24,836 [Thread-1] INFO [org.apache.spark.util.ShutdownHookManager] - Shutdown hook called
2019-10-12 23:40:24,837 [Thread-1] INFO [org.apache.spark.util.ShutdownHookManager] - Deleting directory C:\Users\FrankCooper\AppData\Local\Temp\spark-1f04156b-67d5-4204-9d78-5a45785742f8
